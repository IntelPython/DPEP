{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a002ea61",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c742a",
   "metadata": {},
   "source": [
    "# Few Line Changes to Run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf2099",
   "metadata": {},
   "source": [
    "Let's see the example how you can easily in a few lines of code switch computations from CPU to GPU device.\n",
    "\n",
    "Please look on the original example.\n",
    "We allocate 2 matrices on the Host (CPU) device usnig NumPy array function, all future calculations will be performed as well on the allocated Host(CPU) device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e8711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res =  [[2 2]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "# Original CPU script\n",
    "\n",
    "# Call numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Data alocated on the CPU device\n",
    "x = np.array([[1, 1], [1, 1]])\n",
    "y = np.array([[1, 1], [1, 1]])\n",
    "\n",
    "# Compute performed on the CPU device, where data is allocated\n",
    "res = np.matmul(x, y)\n",
    "\n",
    "print (\"res = \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c3d61",
   "metadata": {},
   "source": [
    "Now let's try to modify our code in a way when all calculations occur on the GPU device.\n",
    "To do it, you need just to switch to the dpnp library and see on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10b7d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array x is located on the device: Device(level_zero:gpu:0)\n",
      "Array y is located on the device: Device(level_zero:gpu:0)\n",
      "res is located on the device: Device(level_zero:gpu:0)\n",
      "res =  [[2 2]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "# Modified XPU script\n",
    "\n",
    "# Drop-in replacement via single line change\n",
    "import dpnp as np\n",
    "\n",
    "# Data alocated on default SYCL device\n",
    "x = np.array([[1, 1], [1, 1]])\n",
    "y = np.array([[1, 1], [1, 1]])\n",
    "\n",
    "# Compute performed on the device, where data is allocated\n",
    "res = np.matmul(x, y)\n",
    "\n",
    "\n",
    "print (\"Array x is located on the device:\", x.device)\n",
    "print (\"Array y is located on the device:\", y.device)\n",
    "print (\"res is located on the device:\", res.device)\n",
    "print (\"res = \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a58f17",
   "metadata": {},
   "source": [
    "As you may see changing only one line of code help us to perform all calculations on the GPU device.\n",
    "In this example np.array() creates an array on the default SYCL* device, which is \"gpu\" on systems with integrated or discrete GPU (it is \"host\" on systems that do not have GPU). The queue associated with this array is now carried with x and y, and np.matmul(x, y) will do matrix product of two arrays x and y, and respective pre-compiled kernel implementing np.matmul() will be submitted to that queue. The result res will be allocated on the device array associated with that queue too.\n",
    "\n",
    "Now let's make a few improvements in our code and see how we can control and specify exact device on which we want to perform our calculations and which USM memory type to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be340585",
   "metadata": {},
   "source": [
    "# dpnp simple examples with popular functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8c3c6",
   "metadata": {},
   "source": [
    "1. Example to return an array with evenly spaced values within a given interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0435d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 µs ± 27.6 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "Result a is located on the device: Device(level_zero:gpu:0)\n",
      "a =  [ 3  9 15 21 27]\n"
     ]
    }
   ],
   "source": [
    "import dpnp as np\n",
    "\n",
    "# Create an array of values from 3 till 30 with step 6\n",
    "a = np.arange(3, 30, step = 6)\n",
    "\n",
    "print (\"Result a is located on the device:\", a.device)\n",
    "print (\"a = \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6765095",
   "metadata": {},
   "source": [
    "In this example np.arange() creates an array on the default SYCL* device, which is \"gpu\" on systems with integrated or discrete GPU (it is \"host\" on systems that do not have GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35081461",
   "metadata": {},
   "source": [
    "2. Example which calculates on the GPU the sum of the array elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e613398f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result x is located on the device: Device(level_zero:gpu:0)\n",
      "Result y is located on the device: Device(level_zero:gpu:0)\n",
      "The sum of the array elements is:  6\n"
     ]
    }
   ],
   "source": [
    "import dpnp as np\n",
    "\n",
    "x = np.empty(3)\n",
    "\n",
    "try:\n",
    "    # Using filter selector strings to specify root devices for a new array\n",
    "    x = np.asarray ([1, 2, 3], device=\"gpu\")\n",
    "    print (\"Result x is located on the device:\", x.device)\n",
    "except:\n",
    "    print (\"GPU device is not available\")\n",
    "\n",
    "# Return the sum of the array elements\n",
    "y = np.sum (x) # Expect 6\n",
    "\n",
    "print (\"Result y is located on the device:\", y.device)\n",
    "print (\"The sum of the array elements is: \", y )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b4915e",
   "metadata": {},
   "source": [
    "In this example np.asarray() creates an array on the default GPU device. The queue associated with this array is now carried with x, and np.sum(x) will derive it from x, and respective pre-compiled kernel implementing np.sum() will be submitted to that queue. The result y will be allocated on the device 0-dimensional array associated with that queue too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f4c62",
   "metadata": {},
   "source": [
    "3. Example of inversion of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b53afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array a is located on the device: Device(level_zero:gpu:0)\n",
      "Result x is located on the device: Device(level_zero:gpu:0)\n",
      "Array x is: [[-2 -2]\n",
      " [-3 -2]\n",
      " [-2 -1]\n",
      " [ 0 -1]]\n"
     ]
    }
   ],
   "source": [
    "import dpnp as np\n",
    "\n",
    "try:\n",
    "    \n",
    "    # Using filter selector strings to specify root devices for an array\n",
    "    a = np.array([[1, 1], [2, 1], [1, 0], [-1, 0]], device = \"gpu\")\n",
    "    print (\"Array a is located on the device:\", a.device)  \n",
    "\n",
    "    # Do inversion of an array \"a\"\n",
    "    x = np.invert(a)\n",
    "\n",
    "    print (\"Result x is located on the device:\", x.device)\n",
    "    print (\"Array x is:\", x) \n",
    "\n",
    "except:\n",
    "    print (\"GPU device is not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f226a",
   "metadata": {},
   "source": [
    "In this example np.array() creates an array on the default GPU device. The queue associated with this array is now carried with a, and np.invert(a) will derive it from a, and respective pre-compiled kernel implementing np.invert() will be submitted to that queue. The result x will be allocated on the device array associated with that queue too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb3b9b",
   "metadata": {},
   "source": [
    "# dpctl simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4b1b5",
   "metadata": {},
   "source": [
    "Here you may find a list of simple examples which explain how to understand how many devices you have in the systen and how to operate with them\n",
    "Let's print the list of all available SYCL devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b890ae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel(R) OpenCL HD Graphics OpenCL 3.0 \n",
      "Intel(R) FPGA Emulation Platform for OpenCL(TM) OpenCL 1.2 Intel(R) FPGA SDK for OpenCL(TM), Version 20.3\n",
      "Intel(R) OpenCL OpenCL 3.0 WINDOWS\n",
      "Intel(R) Level-Zero 1.3\n"
     ]
    }
   ],
   "source": [
    "# See the list of available SYCL platforms and extra metadata about each platform.\n",
    "import dpctl\n",
    "\n",
    "dpctl.lsplatform()  # Print platform information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e8db4",
   "metadata": {},
   "source": [
    "Let's look on the output.\n",
    "On my platform is available OpenCL GPU driver, Intel(R) FPGA Emulation Device, OpenCL CPU driver and Level Zero GPU driver.\n",
    "If i play with verbocity parameter, i can get more information about the devices i have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffcf0cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform  0 ::\n",
      "    Name        Intel(R) OpenCL HD Graphics\n",
      "    Version     OpenCL 3.0 \n",
      "    Vendor      Intel(R) Corporation\n",
      "    Backend     opencl\n",
      "    Num Devices 1\n",
      "      # 0\n",
      "        Name                Intel(R) Iris(R) Xe Graphics\n",
      "        Version             31.0.101.3430\n",
      "        Filter string       opencl:gpu:0\n",
      "Platform  1 ::\n",
      "    Name        Intel(R) FPGA Emulation Platform for OpenCL(TM)\n",
      "    Version     OpenCL 1.2 Intel(R) FPGA SDK for OpenCL(TM), Version 20.3\n",
      "    Vendor      Intel(R) Corporation\n",
      "    Backend     opencl\n",
      "    Num Devices 1\n",
      "      # 0\n",
      "        Name                Intel(R) FPGA Emulation Device\n",
      "        Version             2022.15.11.0.18_160000\n",
      "        Filter string       opencl:accelerator:0\n",
      "Platform  2 ::\n",
      "    Name        Intel(R) OpenCL\n",
      "    Version     OpenCL 3.0 WINDOWS\n",
      "    Vendor      Intel(R) Corporation\n",
      "    Backend     opencl\n",
      "    Num Devices 1\n",
      "      # 0\n",
      "        Name                11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz\n",
      "        Version             2022.15.11.0.18_160000\n",
      "        Filter string       opencl:cpu:0\n",
      "Platform  3 ::\n",
      "    Name        Intel(R) Level-Zero\n",
      "    Version     1.3\n",
      "    Vendor      Intel(R) Corporation\n",
      "    Backend     ext_oneapi_level_zero\n",
      "    Num Devices 1\n",
      "      # 0\n",
      "        Name                Intel(R) Iris(R) Xe Graphics\n",
      "        Version             1.3.23904\n",
      "        Filter string       level_zero:gpu:0\n"
     ]
    }
   ],
   "source": [
    "# See the list of available SYCL platforms and extra metadata about each platform.\n",
    "import dpctl\n",
    "\n",
    "dpctl.lsplatform(2)  # Print platform information with verbocitz level 2 (highest level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63525d3",
   "metadata": {},
   "source": [
    "Having information about available SYCL platforms you can specify which type of devices you want to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ff47e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<dpctl.SyclDevice [backend_type.opencl, device_type.gpu,  Intel(R) Iris(R) Xe Graphics] at 0x1a1eddd72f0>, <dpctl.SyclDevice [backend_type.level_zero, device_type.gpu,  Intel(R) Iris(R) Xe Graphics] at 0x1a1eddd70f0>]\n"
     ]
    }
   ],
   "source": [
    "# See the list of available gpu devices and their extra metadata.\n",
    "import dpctl\n",
    "\n",
    "if dpctl.has_gpu_devices():\n",
    "    print (dpctl.get_devices(device_type='gpu'))\n",
    "else:\n",
    "    print(\"GPU device is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93e7cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<dpctl.SyclDevice [backend_type.opencl, device_type.cpu,  11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz] at 0x1779083bcf0>]\n"
     ]
    }
   ],
   "source": [
    "# See the list of available gpu devices and their extra metadata.\n",
    "import dpctl\n",
    "\n",
    "if dpctl.has_cpu_devices():\n",
    "    print (dpctl.get_devices(device_type='cpu'))\n",
    "else:\n",
    "    print(\"CPU device is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc3f45",
   "metadata": {},
   "source": [
    "And you can make selection of the specific device in your system using the default selctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c068447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name            Intel(R) Iris(R) Xe Graphics\n",
      "    Driver version  1.3.23904\n",
      "    Vendor          Intel(R) Corporation\n",
      "    Filter string   level_zero:gpu:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dpctl\n",
    "\n",
    "try:\n",
    "    # Create a SyclDevice of type GPU based on whatever is returned\n",
    "    # by the SYCL `gpu_selector` device selector class.\n",
    "    gpu = dpctl.select_gpu_device()\n",
    "    gpu.print_device_info() # print GPU device information\n",
    "\n",
    "except:\n",
    "    print (\"GPU device is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378b79d",
   "metadata": {},
   "source": [
    "Or by using the infromation in filter string of the device create abd explicit SyclDevice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad83abb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name            Intel(R) Iris(R) Xe Graphics\n",
      "    Driver version  1.3.23904\n",
      "    Vendor          Intel(R) Corporation\n",
      "    Profile         FULL_PROFILE\n",
      "    Filter string   level_zero:gpu:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dpctl\n",
    "\n",
    "# Create a SyclDevice with an explicit filter string,\n",
    "# in this case the first level_zero gpu device.\n",
    "try:\n",
    "    level_zero_gpu = dpctl.SyclDevice(\"level_zero:gpu:0\")\n",
    "    level_zero_gpu.print_device_info()\n",
    "except:\n",
    "    print(\"The first level_zero GPU device is not available\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadefe0b",
   "metadata": {},
   "source": [
    "Let's check if your gpu device support double precision. To do this we need to selcet gpu device and check the parameter has_aspect_fp64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a94756d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name            Intel(R) Iris(R) Xe Graphics\n",
      "    Driver version  1.3.23904\n",
      "    Vendor          Intel(R) Corporation\n",
      "    Filter string   level_zero:gpu:0\n",
      "\n",
      "Double precision support is False\n"
     ]
    }
   ],
   "source": [
    "import dpctl\n",
    "# Select GPU device and check double precision support\n",
    "try:\n",
    "    gpu = dpctl.select_gpu_device()\n",
    "    gpu.print_device_info()\n",
    "    print(\"Double precision support is\", gpu.has_aspect_fp64)\n",
    "except:\n",
    "    print(\"The GPU device is not available\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
