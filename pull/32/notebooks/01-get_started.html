
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-KVSVYMBQ0W"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-KVSVYMBQ0W');
    </script>
    
    <title>Getting started with Data Parallel Extensions for Python &#8212; Data Parallel Extensions for Python* 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sdc.css" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css" />
    
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/javascript" src="../_static/sidebar.js"></script>
    <script type="text/javascript" src="../_static/copybutton.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Usage of numpy functions in dpnp library" href="02-dpnp_numpy_fallback.html" />
    <link rel="prev" title="Jupyter* Notebooks" href="../jupyter_notebook.html" />
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,600' rel='stylesheet' type='text/css'/>

  </head><body>
<div class="topbar">
  <a class="brand" title="Intel Python projects" href="../other_intel_python_projects.html"></a>
  <a class="brand_sdc" title="Documentation Home" href="../index.html"></a>

  <ul>
    <li><a class="exampleslink" title="Examples" href="../examples.html"></a></li>
    <li><a class="issueslink" title="Issues" href="https://community.intel.com/t5/Intel-Distribution-for-Python/bd-p/distribution-python"></a></li>
    <li><a class="emaillink" title="Email" href="mailto:scripting@intel.com"></a></li>
    <li><a class="homelink" title="GitHub" href="https://github.com/IntelPython/DPEP"></a></li>
    <li>
      
      
<form action="../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
      
    </li>
  </ul>
</div>

<div class="related">
    <h3>Navigation</h3>
    <ul>
      <li class="right">
	<a href="02-dpnp_numpy_fallback.html" title="Usage of numpy functions in dpnp library">
	  next &raquo;
	</a>
      </li>
      <li class="right">
	<a href="../jupyter_notebook.html" title="Jupyter* Notebooks">
	  &laquo; previous
	</a>
	 |
      </li>
      <li>
	<a href="../index.html">Data Parallel Extensions for Python* 0.1 documentation</a>
	 &#187;
      </li>
      <li><a href="../jupyter_notebook.html" accesskey="U">Jupyter* Notebooks</a> &#187;</li>
      
      <li>Getting started with Data Parallel Extensions for Python</li> 
    </ul>
</div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="Getting-started-with-Data-Parallel-Extensions-for-Python">
<h1>Getting started with Data Parallel Extensions for Python<a class="headerlink" href="#Getting-started-with-Data-Parallel-Extensions-for-Python" title="Permalink to this heading">¶</a></h1>
<p><img alt="996d1c408a8d4152a717f7372190ab4c" class="no-scaled-link" src="https://intelpython.github.io/DPEP/main/_images/DPEP-large.png" style="width: 300px;" /></p>
<p><a class="reference external" href="https://intelpython.github.io/DPEP/main/">Data Parallel Extensions for Python</a> allow you to run NumPy-like code beyond CPU using <strong>Data Parallel Extension for NumPy</strong>. It will also allow you to compile the code using <strong>Data Parallel Extension for Numba</strong>.</p>
<section id="Modifying-CPU-script-to-run-on-GPU">
<h2>Modifying CPU script to run on GPU<a class="headerlink" href="#Modifying-CPU-script-to-run-on-GPU" title="Permalink to this heading">¶</a></h2>
<p>In many cases the process of running Python on GPU is about making minor changes to your CPU script, which are: 1. Changing import statement(s) 2. Specifying on which device(s) the data is allocated 3. Explicitly copying data between devices and the host as needed</p>
<p>We will illustrate these concepts on series of short examples. Let’s assume we have the following NumPy script originally written to run on CPU, which is nothing more than creating two matrices <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> and performing matrix-matrix multiplication with <code class="docutils literal notranslate"><span class="pre">numpy.matmul()</span></code>, and prining the resulting matrix:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># CPU script for matrix-matrix multiplication using NumPy

# 1. Import numpy
import numpy as np

# 2. Create two matrices
x = np.array([[1, 1], [1, 1]])
y = np.array([[1, 1], [1, 1]])

# 3. Perform matrix-matrix multiplication
res = np.matmul(x, y)

# 4. Print resulting matrix
print(res)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[2 2]
 [2 2]]
</pre></div></div>
</div>
<p>As stated before in many cases to run the same code on GPU is a trivial modification of a few lines of the code, like this:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Modified script to run the same code on GPU using dpnp

# 1. Import dpnp
import dpnp as np  # Note, we changed the import statement. Since dpnp is a drop-in replacement of numpy the rest of the code will run lik regular numpy

# 2. Create two matrices
x = np.array([[1, 1], [1, 1]])
y = np.array([[1, 1], [1, 1]])

# 3. Perform matrix-matrix multiplication
res = np.matmul(x, y)

# 4. Print what&#39;s going on under the hood
print(res)
print(&quot;Array x is allocated on the device:&quot;, x.device)
print(&quot;Array y is allocated on the device:&quot;, y.device)
print(&quot;res is allocated on the device:&quot;, res.device)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[2 2]
 [2 2]]
Array x is allocated on the device: Device(level_zero:gpu:0)
Array y is allocated on the device: Device(level_zero:gpu:0)
res is allocated on the device: Device(level_zero:gpu:0)
</pre></div></div>
</div>
<p>Let’s see what we actually changed.</p>
<ol class="arabic simple">
<li><p>Obviously we changed the import statement. Now we import <code class="docutils literal notranslate"><span class="pre">dpnp</span></code>, which is a drop-in replacement for a subset of <code class="docutils literal notranslate"><span class="pre">numpy</span></code> that extends numpy-like codes beyond CPU</p></li>
<li><p>No change in matrix creation code. How does <code class="docutils literal notranslate"><span class="pre">dpnp</span></code> know that matrices <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> need to be allocated on GPU? This is because, if we do not specify the device explicitly, the driver will use the default device, which is GPU on systems with installed GPU drivers.</p></li>
<li><p>No change in matrix multiplication code. This is because <code class="docutils literal notranslate"><span class="pre">dpnp</span></code> programming model is the Compute-Follows-Data. It means that <code class="docutils literal notranslate"><span class="pre">dpnp.matmul()</span></code> determines which device will execute an operation based on where array inputs are allocated. Since our inputs <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are allocated on the default device (GPU) the matrix-matrix multiplication will follow data allocation and execute on GPU too.</p></li>
<li><p>Note, arrays <code class="docutils literal notranslate"><span class="pre">x</span></code>, <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">res</span></code> all have the <code class="docutils literal notranslate"><span class="pre">device</span></code> attribute by printing which we make sure that all inputs are indeed on the GPU device, and the result is also on the GPU deivice. To be precise the data allocation (and execution) happened on GPU device 0 through Level-Zero driver.</p></li>
</ol>
</section>
<section id="More-on-data-allocation-and-the-Compute-Follows-Data">
<h2>More on data allocation and the Compute-Follows-Data<a class="headerlink" href="#More-on-data-allocation-and-the-Compute-Follows-Data" title="Permalink to this heading">¶</a></h2>
<p>Sometimes you may want to be specific about the device type, not relying on the default behavior. You can do so by specifying the device in a keyword arguments for <code class="docutils literal notranslate"><span class="pre">dpnp</span></code> array creation functions and random number generators.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import dpnp as np

a = np.arange(3, 30, step = 6, device=&quot;gpu&quot;)
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">dpnp.arange()</span></code> is the array creation function that has optional keyword argument <code class="docutils literal notranslate"><span class="pre">device</span></code>, using which you can specify the device you want data to be allocated on with filter selector string. In our case the string specifies device type <code class="docutils literal notranslate"><span class="pre">&quot;gpu&quot;</span></code>. The proper way of handling situations when the specified device is not available would be as follows:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import dpnp as np

try:
    a = np.arange(3, 30, step = 6, device=&quot;gpu&quot;)
    print(&quot;The a is allocated on the device:&quot;, a.device)
except:
    print(&quot;GPU device is not available&quot;)
    # Do some fallback code

# Do reduction on the selected device
y = np.sum(a)

print(&quot;Reduction sum y: &quot;, y)  # Expect 75
print(&quot;Result y is located on the device:&quot;, y.device)
print(&quot;y.shape=&quot;, y.shape)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The a is allocated on the device: Device(level_zero:gpu:0)
Reduction sum y:  75
Result y is located on the device: Device(level_zero:gpu:0)
y.shape= ()
</pre></div></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">y</span></code> is itself a device array (not a scalar!), its data resides on the same device as input array <code class="docutils literal notranslate"><span class="pre">a</span></code>.</p>
<section id="Advanced-data-and-device-control-with-Data-Parallel-Control-library-dpctl">
<h3>Advanced data and device control with Data Parallel Control library <code class="docutils literal notranslate"><span class="pre">dpctl</span></code><a class="headerlink" href="#Advanced-data-and-device-control-with-Data-Parallel-Control-library-dpctl" title="Permalink to this heading">¶</a></h3>
<p>Data Parallel Control library, <code class="docutils literal notranslate"><span class="pre">dpctl</span></code>, among other things provide advanced capabilities for controling devices and data. Among its useful functions is <code class="docutils literal notranslate"><span class="pre">dpctl.lsplatform(verbosity)</span></code>, printing information about the list of available devices on the system with different levels of verbosity:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import dpctl

dpctl.lsplatform()  # Print platform information
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Intel(R) OpenCL HD Graphics OpenCL 3.0
Intel(R) Level-Zero 1.3
</pre></div></div>
</div>
<p>Using a different verbosity setting to print extra meta-data:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import dpctl

dpctl.lsplatform(2)  # Print platform information with verbocitz level 2 (highest level)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Platform  0 ::
    Name        Intel(R) OpenCL HD Graphics
    Version     OpenCL 3.0
    Vendor      Intel(R) Corporation
    Backend     opencl
    Num Devices 1
      # 0
        Name                Intel(R) UHD Graphics 620
        Version             31.0.101.2111
        Filter string       opencl:gpu:0
Platform  1 ::
    Name        Intel(R) Level-Zero
    Version     1.3
    Vendor      Intel(R) Corporation
    Backend     ext_oneapi_level_zero
    Num Devices 1
      # 0
        Name                Intel(R) UHD Graphics 620
        Version             1.3.0
        Filter string       level_zero:gpu:0
</pre></div></div>
</div>
<p>You can also query whether system has GPU devices and retrieve respective device objects:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import dpctl
import dpnp as np

if dpctl.has_gpu_devices():
    devices = dpctl.get_devices(device_type=&#39;gpu&#39;)
    print(f&quot;This system has {len(devices)} GPUs&quot;)
    for device in devices:
        device.print_device_info()

    x = np.array([1, 2, 3], device=devices[0])  # Another way of selecting on which device to allocate the data
    print(&quot;Array x is on the device:&quot;, x.device)
else:
    print(&quot;GPU devices are not available on this system&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
This system has 2 GPUs
    Name            Intel(R) UHD Graphics 620
    Driver version  31.0.101.2111
    Vendor          Intel(R) Corporation
    Filter string   opencl:gpu:0

    Name            Intel(R) UHD Graphics 620
    Driver version  1.3.0
    Vendor          Intel(R) Corporation
    Filter string   level_zero:gpu:0

Array x is on the device: Device(opencl:gpu:0)
</pre></div></div>
</div>
<p>The following snapshot illustrates how to select the default GPU device using <code class="docutils literal notranslate"><span class="pre">dpctl</span></code> and generate an array of random numbers on this device:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>try:
    gpu = dpctl.select_gpu_device()
    gpu.print_device_info() # print GPU device information
    x = np.random.random(5, device=gpu)  # Create array of random numbers on the default GPU device
    print(&quot;Array x:&quot;, x)
    print(&quot;Array x.device:&quot;, x.device)
except:
    print (&quot;No GPU devices available&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
    Name            Intel(R) UHD Graphics 620
    Driver version  1.3.0
    Vendor          Intel(R) Corporation
    Filter string   level_zero:gpu:0

Array x: [0.87253657 0.87415047 0.61092713 0.1395424  0.95248436]
Array x.device: Device(level_zero:gpu:0)
</pre></div></div>
</div>
<p>Or by creating GPU device object from the filter selector string:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import dpctl

try:
    l0_gpu_0 = dpctl.SyclDevice(&quot;level_zero:gpu:0&quot;)
    l0_gpu_0.print_device_info()
except:
    print(&quot;Cannot create the device object from a given filter selector string&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
    Name            Intel(R) UHD Graphics 620
    Driver version  1.3.0
    Vendor          Intel(R) Corporation
    Filter string   level_zero:gpu:0

</pre></div></div>
</div>
<p>The following snapshot checks whether a given device supports certain aspects, which may be important for the application, such as support for float64 (double precision) or the amount of available global memory on the device, etc.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import dpctl

try:
    gpu = dpctl.select_gpu_device()
    gpu.print_device_info()
    print(&quot;Has double precision:&quot;, gpu.has_aspect_fp64)
    print(&quot;Has atomic operations support:&quot;, gpu.has_aspect_atomic64)
    print(f&quot;Global memory size: {gpu.global_mem_size/1024/1024} MB&quot;)
    print(f&quot;Global memory cache size: {gpu.global_mem_cache_size/1024} KB&quot;)
    print(f&quot;Maximum compute units: {gpu.max_compute_units}&quot;)
except:
    print(&quot;The GPU device is not available&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
    Name            Intel(R) UHD Graphics 620
    Driver version  1.3.0
    Vendor          Intel(R) Corporation
    Filter string   level_zero:gpu:0

Has double precision: True
Has atomic operations support: True
Global memory size: 6284.109375 MB
Global memory cache size: 512.0 KB
Maximum compute units: 24
</pre></div></div>
</div>
<p>For more information about <code class="docutils literal notranslate"><span class="pre">dpctl</span></code> device selection please refer to <a class="reference external" href="https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/device_selection.html">Data Parallel Control: Device Selection</a></p>
<p>For more information about <code class="docutils literal notranslate"><span class="pre">dpctl.SyclDevice</span></code> class methods and attributes please refer to <a class="reference external" href="https://intelpython.github.io/dpctl/latest/docfiles/dpctl/SyclDevice.html">Data Parallel Control: dpctl.SyclDevice</a></p>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3>Table of Contents</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../prerequisites_and_installation.html">Prerequisites and installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallelism.html">Parallelism in modern data parallel architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../heterogeneous_computing.html">Heterogeneous computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming_dpep.html">Programming with Data Parallel Extensions for Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">List of examples</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../jupyter_notebook.html">Jupyter* Notebooks</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="02-dpnp_numpy_fallback.html">Controlling `dpnp` fallback to `numpy`</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../useful_links.html">Useful links</a></li>
<li class="toctree-l1"><a class="reference internal" href="../useful_links.html#to-do">To-Do</a></li>
</ul>


<form action="../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="../jupyter_notebook.html"
                          title="previous chapter">Jupyter* Notebooks</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="02-dpnp_numpy_fallback.html"
                          title="next chapter">Usage of numpy functions in dpnp library</a></p>
  </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
<footer class="footer">
  <p class="pull-right"> &nbsp;
    <a href="#">Back to Top</a></p>
  <p>
    &copy; Copyright 2020-2023, Intel Corporation.<br/>
    Created using <a href="http://www.sphinx-doc.org/en/stable/">Sphinx</a> 6.1.3. &nbsp;
  </p>
</footer>
  </body>
</html>