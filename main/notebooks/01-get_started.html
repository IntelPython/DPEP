<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-KVSVYMBQ0W"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-KVSVYMBQ0W');
    </script>
    
    <title>Jupyter Notebook 1: Getting started with Data Parallel Extensions for Python &#8212; Data Parallel Extensions for Python* 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/sdc.css?v=8cc1021e" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css" />
    
    <script src="../_static/documentation_options.js?v=2709fde1"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/javascript" src="../_static/sidebar.js"></script>
    <script type="text/javascript" src="../_static/copybutton.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Jupyter Notebook 2: Falling back to NumPy" href="02-dpnp_numpy_fallback.html" />
    <link rel="prev" title="Tutorials" href="../jupyter_notebook.html" />
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,600' rel='stylesheet' type='text/css'/>

  </head><body>
<div class="topbar">
  <a class="brand" title="Intel Python projects" href="../other_intel_python_projects.html"></a>
  <a class="brand_sdc" title="Documentation Home" href="../index.html"></a>

  <ul>
    <li><a class="exampleslink" title="Examples" href="../examples.html"></a></li>
    <li><a class="issueslink" title="Issues" href="https://community.intel.com/t5/Intel-Distribution-for-Python/bd-p/distribution-python"></a></li>
    <li><a class="emaillink" title="Email" href="mailto:scripting@intel.com"></a></li>
    <li><a class="homelink" title="GitHub" href="https://github.com/IntelPython/DPEP"></a></li>
    <li>
      
      
<form action="../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
      
    </li>
  </ul>
</div>

<div class="related">
    <h3>Navigation</h3>
    <ul>
      <li class="right">
	<a href="02-dpnp_numpy_fallback.html" title="Jupyter Notebook 2: Falling back to NumPy">
	  next &raquo;
	</a>
      </li>
      <li class="right">
	<a href="../jupyter_notebook.html" title="Tutorials">
	  &laquo; previous
	</a>
	 |
      </li>
      <li>
	<a href="../index.html">Data Parallel Extensions for Python* 0.1 documentation</a>
	 &#187;
      </li>
      <li><a href="../jupyter_notebook.html" accesskey="U">Tutorials</a> &#187;</li>
      
      <li>Jupyter Notebook 1: Getting started with Data Parallel Extensions for Python</li> 
    </ul>
</div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="Jupyter-Notebook-1:-Getting-started-with-Data-Parallel-Extensions-for-Python">
<h1>Jupyter Notebook 1: Getting started with Data Parallel Extensions for Python<a class="headerlink" href="#Jupyter-Notebook-1:-Getting-started-with-Data-Parallel-Extensions-for-Python" title="Link to this heading">¶</a></h1>
<p><img alt="cc54eb7cec074b4d817607443095401d" class="no-scaled-link" src="https://intelpython.github.io/DPEP/main/_images/DPEP-large.png" style="width: 300px;" /></p>
<p><a class="reference external" href="https://intelpython.github.io/DPEP/main/">Data Parallel Extensions for Python</a> allow you to run NumPy-like code beyond CPU using <strong>Data Parallel Extension for NumPy</strong>. It will also allow you to compile the code using <strong>Data Parallel Extension for Numba</strong>.</p>
<section id="Modifying-CPU-script-to-run-on-GPU">
<h2>Modifying CPU script to run on GPU<a class="headerlink" href="#Modifying-CPU-script-to-run-on-GPU" title="Link to this heading">¶</a></h2>
<p>In many cases the process of running Python on GPU is about making minor changes to your CPU script, which are: 1. Changing import statement(s) 2. Specifying on which device(s) the data is allocated 3. Explicitly copying data between devices and the host as needed</p>
<p>We will illustrate these concepts on series of short examples. Let’s assume we have the following NumPy script originally written to run on CPU, which is nothing more than creating two matrices <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> and performing matrix-matrix multiplication with <code class="docutils literal notranslate"><span class="pre">numpy.matmul()</span></code>, and prining the resulting matrix:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># CPU script for matrix-matrix multiplication using NumPy</span>

<span class="c1"># 1. Import numpy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 2. Create two matrices</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># 3. Perform matrix-matrix multiplication</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># 4. Print resulting matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[2 2]
 [2 2]]
</pre></div></div>
</div>
<p>As stated before in many cases to run the same code on GPU is a trivial modification of a few lines of the code, like this:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Modified script to run the same code on GPU using dpnp</span>

<span class="c1"># 1. Import dpnp</span>
<span class="kn">import</span> <span class="nn">dpnp</span> <span class="k">as</span> <span class="nn">np</span>  <span class="c1"># Note, we changed the import statement. Since dpnp is a drop-in replacement of numpy the rest of the code will run lik regular numpy</span>

<span class="c1"># 2. Create two matrices</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># 3. Perform matrix-matrix multiplication</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># 4. Print what&#39;s going on under the hood</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Array x is allocated on the device:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Array y is allocated on the device:&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;res is allocated on the device:&quot;</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[2 2]
 [2 2]]
Array x is allocated on the device: Device(level_zero:gpu:0)
Array y is allocated on the device: Device(level_zero:gpu:0)
res is allocated on the device: Device(level_zero:gpu:0)
</pre></div></div>
</div>
<p>Let’s see what we actually changed.</p>
<ol class="arabic simple">
<li><p>Obviously we changed the import statement. Now we import <code class="docutils literal notranslate"><span class="pre">dpnp</span></code>, which is a drop-in replacement for a subset of <code class="docutils literal notranslate"><span class="pre">numpy</span></code> that extends numpy-like codes beyond CPU</p></li>
<li><p>No change in matrix creation code. How does <code class="docutils literal notranslate"><span class="pre">dpnp</span></code> know that matrices <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> need to be allocated on GPU? This is because, if we do not specify the device explicitly, the driver will use the default device, which is GPU on systems with installed GPU drivers.</p></li>
<li><p>No change in matrix multiplication code. This is because <code class="docutils literal notranslate"><span class="pre">dpnp</span></code> programming model is the Compute-Follows-Data. It means that <code class="docutils literal notranslate"><span class="pre">dpnp.matmul()</span></code> determines which device will execute an operation based on where array inputs are allocated. Since our inputs <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are allocated on the default device (GPU) the matrix-matrix multiplication will follow data allocation and execute on GPU too.</p></li>
<li><p>Note, arrays <code class="docutils literal notranslate"><span class="pre">x</span></code>, <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">res</span></code> all have the <code class="docutils literal notranslate"><span class="pre">device</span></code> attribute by printing which we make sure that all inputs are indeed on the GPU device, and the result is also on the GPU deivice. To be precise the data allocation (and execution) happened on GPU device 0 through Level-Zero driver.</p></li>
</ol>
</section>
<section id="More-on-data-allocation-and-the-Compute-Follows-Data">
<h2>More on data allocation and the Compute-Follows-Data<a class="headerlink" href="#More-on-data-allocation-and-the-Compute-Follows-Data" title="Link to this heading">¶</a></h2>
<p>Sometimes you may want to be specific about the device type, not relying on the default behavior. You can do so by specifying the device in a keyword arguments for <code class="docutils literal notranslate"><span class="pre">dpnp</span></code> array creation functions and random number generators.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dpnp</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">dpnp.arange()</span></code> is the array creation function that has optional keyword argument <code class="docutils literal notranslate"><span class="pre">device</span></code>, using which you can specify the device you want data to be allocated on with filter selector string. In our case the string specifies device type <code class="docutils literal notranslate"><span class="pre">&quot;gpu&quot;</span></code>. The proper way of handling situations when the specified device is not available would be as follows:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dpnp</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The a is allocated on the device:&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU device is not available&quot;</span><span class="p">)</span>
    <span class="c1"># Do some fallback code</span>

<span class="c1"># Do reduction on the selected device</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reduction sum y: &quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># Expect 75</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result y is located on the device:&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y.shape=&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The a is allocated on the device: Device(level_zero:gpu:0)
Reduction sum y:  75
Result y is located on the device: Device(level_zero:gpu:0)
y.shape= ()
</pre></div></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">y</span></code> is itself a device array (not a scalar!), its data resides on the same device as input array <code class="docutils literal notranslate"><span class="pre">a</span></code>.</p>
<section id="Advanced-data-and-device-control-with-Data-Parallel-Control-library-dpctl">
<h3>Advanced data and device control with Data Parallel Control library <code class="docutils literal notranslate"><span class="pre">dpctl</span></code><a class="headerlink" href="#Advanced-data-and-device-control-with-Data-Parallel-Control-library-dpctl" title="Link to this heading">¶</a></h3>
<p>Data Parallel Control library, <code class="docutils literal notranslate"><span class="pre">dpctl</span></code>, among other things provide advanced capabilities for controling devices and data. Among its useful functions is <code class="docutils literal notranslate"><span class="pre">dpctl.lsplatform(verbosity)</span></code>, printing information about the list of available devices on the system with different levels of verbosity:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dpctl</span>

<span class="n">dpctl</span><span class="o">.</span><span class="n">lsplatform</span><span class="p">()</span>  <span class="c1"># Print platform information</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Intel(R) OpenCL HD Graphics OpenCL 3.0
Intel(R) Level-Zero 1.3
</pre></div></div>
</div>
<p>Using a different verbosity setting to print extra meta-data:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dpctl</span>

<span class="n">dpctl</span><span class="o">.</span><span class="n">lsplatform</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Print platform information with verbocitz level 2 (highest level)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Platform  0 ::
    Name        Intel(R) OpenCL HD Graphics
    Version     OpenCL 3.0
    Vendor      Intel(R) Corporation
    Backend     opencl
    Num Devices 1
      # 0
        Name                Intel(R) UHD Graphics 620
        Version             31.0.101.2111
        Filter string       opencl:gpu:0
Platform  1 ::
    Name        Intel(R) Level-Zero
    Version     1.3
    Vendor      Intel(R) Corporation
    Backend     ext_oneapi_level_zero
    Num Devices 1
      # 0
        Name                Intel(R) UHD Graphics 620
        Version             1.3.0
        Filter string       level_zero:gpu:0
</pre></div></div>
</div>
<p>You can also query whether system has GPU devices and retrieve respective device objects:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dpctl</span>
<span class="kn">import</span> <span class="nn">dpnp</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">if</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">has_gpu_devices</span><span class="p">():</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">get_devices</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="s1">&#39;gpu&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;This system has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span><span class="si">}</span><span class="s2"> GPUs&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
        <span class="n">device</span><span class="o">.</span><span class="n">print_device_info</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># Another way of selecting on which device to allocate the data</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Array x is on the device:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU devices are not available on this system&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
This system has 2 GPUs
    Name            Intel(R) UHD Graphics 620
    Driver version  31.0.101.2111
    Vendor          Intel(R) Corporation
    Filter string   opencl:gpu:0

    Name            Intel(R) UHD Graphics 620
    Driver version  1.3.0
    Vendor          Intel(R) Corporation
    Filter string   level_zero:gpu:0

Array x is on the device: Device(opencl:gpu:0)
</pre></div></div>
</div>
<p>The following snapshot illustrates how to select the default GPU device using <code class="docutils literal notranslate"><span class="pre">dpctl</span></code> and generate an array of random numbers on this device:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">gpu</span> <span class="o">=</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">select_gpu_device</span><span class="p">()</span>
    <span class="n">gpu</span><span class="o">.</span><span class="n">print_device_info</span><span class="p">()</span> <span class="c1"># print GPU device information</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">gpu</span><span class="p">)</span>  <span class="c1"># Create array of random numbers on the default GPU device</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Array x:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Array x.device:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;No GPU devices available&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
    Name            Intel(R) UHD Graphics 620
    Driver version  1.3.0
    Vendor          Intel(R) Corporation
    Filter string   level_zero:gpu:0

Array x: [0.87253657 0.87415047 0.61092713 0.1395424  0.95248436]
Array x.device: Device(level_zero:gpu:0)
</pre></div></div>
</div>
<p>Or by creating GPU device object from the filter selector string:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dpctl</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">l0_gpu_0</span> <span class="o">=</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">SyclDevice</span><span class="p">(</span><span class="s2">&quot;level_zero:gpu:0&quot;</span><span class="p">)</span>
    <span class="n">l0_gpu_0</span><span class="o">.</span><span class="n">print_device_info</span><span class="p">()</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cannot create the device object from a given filter selector string&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
    Name            Intel(R) UHD Graphics 620
    Driver version  1.3.0
    Vendor          Intel(R) Corporation
    Filter string   level_zero:gpu:0

</pre></div></div>
</div>
<p>The following snapshot checks whether a given device supports certain aspects, which may be important for the application, such as support for float64 (double precision) or the amount of available global memory on the device, etc.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dpctl</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">gpu</span> <span class="o">=</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">select_gpu_device</span><span class="p">()</span>
    <span class="n">gpu</span><span class="o">.</span><span class="n">print_device_info</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Has double precision:&quot;</span><span class="p">,</span> <span class="n">gpu</span><span class="o">.</span><span class="n">has_aspect_fp64</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Has atomic operations support:&quot;</span><span class="p">,</span> <span class="n">gpu</span><span class="o">.</span><span class="n">has_aspect_atomic64</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Global memory size: </span><span class="si">{</span><span class="n">gpu</span><span class="o">.</span><span class="n">global_mem_size</span><span class="o">/</span><span class="mi">1024</span><span class="o">/</span><span class="mi">1024</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Global memory cache size: </span><span class="si">{</span><span class="n">gpu</span><span class="o">.</span><span class="n">global_mem_cache_size</span><span class="o">/</span><span class="mi">1024</span><span class="si">}</span><span class="s2"> KB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Maximum compute units: </span><span class="si">{</span><span class="n">gpu</span><span class="o">.</span><span class="n">max_compute_units</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The GPU device is not available&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
    Name            Intel(R) UHD Graphics 620
    Driver version  1.3.0
    Vendor          Intel(R) Corporation
    Filter string   level_zero:gpu:0

Has double precision: True
Has atomic operations support: True
Global memory size: 6284.109375 MB
Global memory cache size: 512.0 KB
Maximum compute units: 24
</pre></div></div>
</div>
<p>For more information about <code class="docutils literal notranslate"><span class="pre">dpctl</span></code> device selection please refer to <a class="reference external" href="https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/device_selection.html">Data Parallel Control: Device Selection</a></p>
<p>For more information about <code class="docutils literal notranslate"><span class="pre">dpctl.SyclDevice</span></code> class methods and attributes please refer to <a class="reference external" href="https://intelpython.github.io/dpctl/latest/docfiles/dpctl/SyclDevice.html">Data Parallel Control: dpctl.SyclDevice</a></p>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3>Table of Contents</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../prerequisites_and_installation.html">Prerequisites and Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallelism.html">Parallelism in Modern Data-Parallel Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../heterogeneous_computing.html">Heterogeneous Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming_dpep.html">Programming with Data Parallel Extensions for Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../jupyter_notebook.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="02-dpnp_numpy_fallback.html">Controlling `dpnp` fallback to `numpy`</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demos.html">Demos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../useful_links.html">Useful links</a></li>
<li class="toctree-l1"><a class="reference internal" href="../useful_links.html#to-do">To-Do</a></li>
</ul>

<form action="../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="../jupyter_notebook.html"
                          title="previous chapter">Tutorials</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="02-dpnp_numpy_fallback.html"
                          title="next chapter">Jupyter Notebook 2: Falling back to NumPy</a></p>
  </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
<footer class="footer">
  <p class="pull-right"> &nbsp;
    <a href="#">Back to Top</a></p>
  <p>
    &copy; Copyright 2020-2023, Intel Corporation.<br/>
    Created using <a href="http://www.sphinx-doc.org/en/stable/">Sphinx</a> 7.2.6. &nbsp;
  </p>
</footer>
  </body>
</html>